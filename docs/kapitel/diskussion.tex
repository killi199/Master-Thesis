% TODO ggf. Sagen, warum nicht auf die .all-contributorsrc-Datei eingegangen wurde -> README wird analysiert dadurch Aufwand gespart könnte jedoch noch verbessert werden in dem Auch die Datei zusätzlich noch analysiert wird, da dort noch mehr informationen wie der Beitrag gelistet werden.
% TODO ggf. erklären warum die API verwendet wird und nicht nur die TOML beispielsweise ausgelesen wird -> habe ein pypi paket und brauche die GitHub-URL (vllt eher diskussion oder so? Beschreibe hier ja nur was ich mache und nicht warum)
% TODO ggf. Unterschiedliche Programme unterschiedliche Commit Anzahl 
% Nochmal checken warum unterschiedliche Programme unterschiedliche Anzahlen an Commits ausgeben dafür eine Begründung suchen und das beste System auswählen -> wahrscheinlich ist es GitHub was aber blöd ist.
% GitHub hat andere Anzahl, da sie automatisch über die E-Mails machten, welche in GitHub registriert sind. Ich gruppiere nur die gleichen E-Mails.
% matplotlib Antony Lee:
%   git-quick-stats: 3864 (merges nicht inbegriffen https://github.com/arzzen/git-quick-stats?tab=readme-ov-file#git-merge-view-strategy):
%   GitHub: 4476 (merges nicht inbegriffen https://docs.github.com/en/repositories/viewing-activity-and-data-for-your-repository/viewing-a-projects-contributors#about-contributors gefühlt aber doch inbegriffen....) (E-Mails werden gemerged die GitHub bekannt sind)
%   git shortlog -s -n --no-merges: 3844 (merges nicht inbegriffen) 4370 (merges inbegriffen)
%   git fame: 4370 (merges inbegriffen)

\chapter{Diskussion}
\label{chap:diskussion}
In diesem Kapitel wird die Arbeit diskutiert und die Ergebnisse interpretiert.
Zuerst wird auf die grundlegenden Limitierungen dieser Arbeit eingegangen und wie diese die Ergebnisse beeinflussen.
Diese Limitierungen können nicht vollständig behoben werden.
Anschließend erfolgt eine Diskussion über die Qualität des Abgleichs der Autoren.
Daraufhin wird diskutiert, was Softwareentwickler leisten müssen, um zitiert zu werden.
Natürlich lassen sich keine allgemeinen Aussagen für alle Pakete treffen, da dies individuell unterschiedlich ist.
Zuletzt wird erörtert, wie sorgfältig die Autoren in den untersuchten Listen gepflegt sind.

\section{Limitierungen}
\label{sec:limitierungen}
\subsection*{Git Statistik}
\label{subsec:git_statistik}
In \autoref{sec:versionsverwaltung} wurde beschrieben, dass Autoren in Git ihren Namen und ihre E-Mail-Adresse ohne Einschränkungen eigenständig eintragen können.
Dies bedeutet ebenfalls, dass ein Autor im Verlauf der Zeit seinen Namen und/ oder die E-Mail-Adresse ändern kann.
Dadurch kann ein und dieselbe Person mit unterschiedlichen Git Namen unterschiedlich viele Commits erstellt haben.
Dies ist für die Masterarbeit nicht erwünscht, da möglichst ein Autor nur einmal in den Daten vorkommen soll und sämtliche Commits diesem Autor angerechnet werden sollen.
In der Datenbeschaffung wurde versucht, dieses Problem zu lösen, indem die E-Mail-Adresse als eindeutiger Identifikator genutzt wird und gleiche E-Mail-Adressen zu einem Autor zusammengefasst werden.
Allerdings behebt dies nicht das Problem vollständig, da ein Autor seine E-Mail-Adresse ebenfalls ändern kann und diese nicht erneut abgeglichen wird.

Ein Abgleich, wie in \autoref{sec:abgleich} beschrieben, bei dem keine Unterscheidung von Namensvettern möglich ist, ist ebenfalls nicht möglich. 
Dies ist darin begründet, dass dadurch die Ergebnisse verfälscht werden könnten, da beispielsweise Autoren mit dem gleichen Namen zusammengefasst werden würden, obwohl es sich um unterschiedliche Autoren handelt.
Eine Möglichkeit, dieses Problem zu lösen, wäre die Verwendung der GitHub-API, welche automatisch die Git-Autoren anhand der in GitHub eingetragenen E-Mail-Adressen zusammenfasst.
Ein weiterer Vorteil, der durch die Nutzung der GitHub-API bestehen würde, ist, dass dabei die Benutzernamen der GitHub-Benutzer abgefragt werden könnten.
Diese werden häufig in der README und der Beschreibung mit einem @-Zeichen angegeben, um auf diese zu verweisen.
Die verwendete \gls{ner} erkennt die Benutzernamen, allerdings können sie häufig nicht mit den Git-Daten abgeglichen werden, da die Benutzernamen meistens nicht dem richtigen Namen oder der E-Mail-Adresse entsprechen.
Die Verwendung der API ist jedoch nur mit viel Zeit möglich, da sie für jedes Paket einzeln abgefragt werden müsste und dadurch schnell das Ratenlimit von GitHub erreicht werden würde.
Eine weitere Möglichkeit, welche bereits durch \emph{git-quick-stats} verwendet wird, ist das Zusammenfassen der Autoren mittels einer \texttt{.mailmap}-Datei \autocite{chacon_git_2024-1}.
In dieser Datei kann eingetragen werden, dass zwei E-Mail-Adressen zusammengefasst werden sollen.

Durch die beschriebene Limitierung kommt es beispielsweise vor, dass in \emph{torch} sechsmal der Autor \glqq Edward Yang\grqq{} mit unterschiedlichen E-Mail-Adressen vorkommt, obwohl es sich um die gleiche Person handelt.
Der erste Eintrag des Autors hat 1.925 Commits, alle weiteren Einträge zusammen haben nochmal 1.282 Commits getätigt.
Diese Limitierung in der Datenbeschaffung verfälscht die Gesamtergebnisse.
Aber auch bei einem erneuten Abgleich würde dies die Ergebnisse durch die Ungenauigkeit des Abgleichs verfälschen.

\subsection*{Autoren ohne Commits}
\label{subsec:autoren_ohne_commits}
Eine weitere Limitierung, welche bereits im Verlauf dieser Masterarbeit häufiger thematisiert wurde, ist, dass Autoren als Autor genannt werden können und auch sollten, obwohl sie keinen Quellcode geschrieben haben.
Eine reine Betrachtung der geleisteten Arbeit anhand der Änderungen am Quellcode, wie sie in dieser Masterarbeit durchgeführt wurde, ist daher nicht ausreichend, um das gesamte Spektrum der Arbeit von Autoren zu erfassen.
Aus diesem Grund kommt es in dieser Arbeit dazu, dass Autoren im Abgleich nicht erkannt werden, da sie keinen Quellcode geschrieben haben und somit nicht als Git-Autoren gelistet sind.
Dadurch können Ergebnisse wie beispielsweise in \autoref{fig:common_authors_2} schlechter ausfallen, da nur auf Git-Autoren geprüft wird.
Es werden keine Autoren berücksichtigt, welche beispielsweise an der Dokumentation gearbeitet haben oder an der Organisation des Projekts beteiligt waren.

\subsection*{Verlinkung auf andere Quellen}
\label{subsec:verlinkung_auf_andere_quellen}
In einigen Quellen werden Autoren durch die Entwickler nicht direkt genannt, sondern es wird auf eine andere Quelle verwiesen, in welcher die Autoren genannt werden.
Beispielsweise wird in \emph{pandas} in der \gls{cff}-Datei als Name \glqq The pandas development team\grqq{} und anschließend die Webseite \url{https://pandas.pydata.org/about/team.html} angegeben.
Auf dieser Seite sind anschließend alle aktiven Autoren gelistet.
In dieser Masterarbeit stehen lediglich die Autoren in der \gls{cff}-Datei mit Vor- und Nachnamen im Fokus.
Es erfolgt keine zusätzliche Analyse anderer Quellen, die möglicherweise erwähnt sind.
Daher kann es wie im Fall von \emph{pandas} passieren, dass Autoren, die nicht direkt genannt sind, jedoch auf einer anderen Seite gelistet werden, unberücksichtigt bleiben.
Dies schränkt die Ergebnisse dieser Masterarbeit ein, da nicht alle Autoren einbezogen sind.
Eine Betrachtung der verlinkten Seiten würde jedoch einen erheblichen Mehraufwand bedeuten, da für jedes Paket die verlinkten Seiten analysiert werden müssten.
Außerdem sind die verlinkten Seiten unterschiedlich aufgebaut, sodass die Komplexität dadurch ebenfalls gesteigert wäre.
Aus diesem Grund wurde in dieser Masterarbeit darauf verzichtet, die verlinkten Seiten zu analysieren und mit weniger genannten Autoren gearbeitet.

\subsection*{Ausführungszeit der Datenbeschaffung}
\label{subsec:ausfuehrungszeit_der_datenbeschaffung}
Eine weitere Limitierung ist die Laufzeit der Datenbeschaffung.
Diese wird besonders durch den Aufruf von \emph{git-quick-stats} beeinflusst, da das Programm für große Repositorys eine lange Laufzeit hat.
Beispielsweise benötigt die Ausführung des Programms für das Paket \emph{pytorch} 1 Minute und 22 Sekunden.
Außerdem hat die \gls{ner} für die README und die Beschreibung eine hohe Laufzeit, weswegen für die README nur die letzten 50 Commits betrachtet werden.
Die Laufzeit der \gls{ner} beträgt für die README von \emph{pytorch} 3,3588 Sekunden.
Bei einer Ausführung der \gls{ner} für 50 Versionen der README entspricht dies bereits einer Laufzeit von 2 Minuten und 80 Sekunden.

Um die Laufzeit weiter zu reduzieren, beispielsweise um sämtliche \gls{cff} Pakete auf GitHub analysieren zu können, wurde auf Faktoren, welche die Laufzeit erheblich erhöhen, verzichtet.
Aus diesem Grund wurden bei der Analyse ausschließlich die \gls{cff}-Dateien betrachtet.
Zudem wurden für den Abgleich die Git-Autoren benötigt, welche ausschließlich in der neuesten Version beschafft wurden, sodass \emph{git-quick-stats} nur einmal aufgerufen werden musste.
Dadurch konnte die Laufzeit der Datenbeschaffung für die gesamte \gls{cff} Liste mit 20.870 Einträgen auf dem internen HPC Server der Hochschule Wismar auf 55 Stunden reduziert werden.
Der Server hat zwei Intel Xeon Gold 6346 Prozessoren mit jeweils 3,1 GHz je 16 Kerne verbaut.
Bei Betrachtung aller Quellen hätte dieser Prozess erheblich mehr Zeit in Anspruch genommen.
Allerdings muss berücksichtigt werden, dass das Programm nicht täglich, sondern einmalig ausgeführt wird, um die Daten zu einem bestimmten Zeitpunkt zu beschaffen.
Falls ausgelassene Daten für künftige Arbeiten benötigt werden, könnten diese dem Prozess erneut ergänzt werden.

\section{Wie gut können Autoren untereinander abgeglichen werden?}
\label{sec:abgleich_diskussion}
In den Tabellen \ref{tab:matching_results_auto}, \ref{tab:matching_results_auto_anhang}, \ref{tab:matching_results_manual},\ref{tab:cran_matching_results_manual_anhang}, \ref{tab:pypi_matching_results_manual_anhang}, \ref{tab:cff_matching_results_manual_anhang}, \ref{tab:pypi_cff_matching_results_manual_anhang} und \ref{tab:cran_cff_matching_results_manual_anhang} wurden die Ergebnisse des Abgleichs der Autoren dargestellt.
In \autoref{tab:matching_results_manual} fällt auf, dass viele Autoren in den Python Quellen keine Personen sind.
Dies ist darauf zurückzuführen, dass in den Quellen häufig Organisationen als Autoren genannt werden und keine individuellen Personen aufgeführt werden.
Beispielsweise sind in der \gls{pypi} Liste vier Pakete enthalten, welche bereits den Namen Google enthalten.
In allen vier Paketen sind in den Quellen \gls{pypi} Maintainer und Python Autoren keine Personen, sondern ausschließlich \glqq gcloudpypi\grqq{}, \glqq google\_opensource\grqq{} und \glqq Google LLC\grqq{} genannt.
Hier lässt sich diskutieren, ob eine Nennung von individuellen Personen dennoch erfolgen sollte, auch wenn sie beispielsweise bei einem Unternehmen wie Google angestellt sind und für diese arbeiten.
Dies soll allerdings kein Thema für diese Arbeit sein.

In \autoref{tab:matching_results_manual} fällt auf, dass die README und die Beschreibung schlechte F1-Scores haben.
Dies liegt daran, dass die \gls{ner} viele Ergebnisse liefert, unter anderem \gls{fp}, welche anschließend primär falsch zugeordnet werden.
Ebenfalls sind viele \gls{fn} Ergebnisse enthalten, da die \gls{ner} ebenfalls Benutzernamen erkennt, wie bereits erläutert wurde.
Außerdem fällt auf, dass viele \gls{tn} in den Ergebnissen enthalten sind.
Dies ist darauf zurückzuführen, dass die verwendete \gls{ner} viele Entitäten erkennt, welche nicht relevant sind, da es sich beispielsweise nicht um Personen handelt.
Dies ist besonders verwunderlich, da in der Methodik beschrieben wurde, dass die \gls{ner} nur Personen erkennen soll.

Zudem ist in \autoref{tab:matching_results_manual} aufgefallen, dass die \hologo{BibTeX}-Quelle den schlechtesten F1-Score hat.
Dies ist darin begründet, dass nur die ersten beiden Autoren in jeder \hologo{BibTeX}-Datei betrachtet wurden.
Da in allen Listen insgesamt nur vier \hologo{BibTeX}-Dateien enthalten sind, ist die Anzahl der betrachteten Autoren auf maximal acht Autoren begrenzt.
Wie in den Tabellen \ref{tab:matching_results_auto} und \ref{tab:matching_results_auto_anhang} erkennbar ist, sind insgesamt 63 in den \hologo{BibTeX}-Dateien enthalten.
In zwei der 4 Dateien konnten alle Autoren, was in diesen Fällen jeweils ein Autor entspricht, nicht abgeglichen werden.
In den anderen beiden Dateien sind mehr Autoren enthalten, wobei viele der Autoren richtig abgeglichen werden konnten.
Bei einer Betrachtung aller Autoren wäre der F1-Score für die \hologo{BibTeX}-Quelle besser ausgefallen.
In diesem Fall verschlechtern die beiden nicht abgeglichenen Autoren aufgrund der insgesamt geringen Anzahl an betrachteten Autoren den F1-Score erheblich.

Außerdem fällt auf, dass insgesamt viele \gls{fp} in den Ergebnissen enthalten sind.
Diese sind dadurch begründet, dass das Keyword \emph{in} in \autoref{sec:abgleich} verwendet wird.
Bei der manuellen Überprüfung der Ergebnisse fällt auf, dass in einigen Git-Autorenlisten Autoren enthalten sind, welche einen Namen mit nur einem oder zwei Buchstaben haben.
In diesen Fällen ist es möglich, fast jeden Autor aus der Quelle mit diesem speziellen Git Autor abzugleichen, da der Autor der Quelle nur diesen Buchstaben in seinem Namen enthalten haben muss.
Falls über keine weiteren Attribute der Abgleich erfolgen kann, bedeutet dies immer, dass ein falscher Abgleich stattfindet.
Dieses Problem entsteht grundlegend dadurch, dass es kaum Einschränkungen bei der Wahl des Namens und der E-Mail-Adresse in Git gibt.
Außerdem führt das Problem im Allgemeinen zu schlechteren Ergebnissen im Abgleich, da die zu untersuchenden Daten nicht standardisiert sind.

Im Allgemeinen konnte gezeigt werden, dass der Abgleich der Autoren gut funktioniert hat, da ein F1-Score von über 0,9 als gut zu bewerten ist.
Außerdem wurde in diesem Abschnitt diskutiert, warum einige Quellen schlechtere Ergebnisse haben.
Hierbei wurde deutlich, dass die schlechteren Ergebnisse primär nicht durch den Abgleich verursacht wurden, sondern durch andere Faktoren, wie beispielsweise die \gls{ner} oder die Anzahl der manuell betrachteten Autoren in den \hologo{BibTeX}-Dateien.
Bei einer Betrachtung des F1-Scores ohne diese Gegebenheiten würde dieser nochmals verbessert werden.

\section{Was muss ein Softwareentwickler leisten, um als Autor genannt zu werden?}
\label{sec:zitationsfaehiger_autor_diskussion}
Bei der Beantwortung der Frage muss beachtet werden, dass die Aussagen nur allgemein getroffen werden können und nicht für alle Pakete gelten.
Einzelne Pakete können natürlich unterschiedlich sein und andere Anforderungen an die Autoren stellen.
Nur weil ein Paket alle Autoren nennt, welche mindestens einen Commit getätigt haben, bedeutet das nicht, dass ein anderes Paket ebenfalls diesen Ansatz verfolgt.
Die Ergebnisse in \autoref{chap:ergebnisse} zeigen im Allgemeinen aggregierte Werte für alle Pakete einer Liste und aus diesem Grund wird die Frage ebenfalls allgemeingültig diskutiert.

Die \autoref{fig:common_authors} zeigt, dass Autoren mit vielen Commits über alle Pakete hinweg häufiger als Autoren genannt sind.
Sie zeigt, dass eine erhöhte Chance besteht, falls eine Person unter den Top-10-Autoren ist, dass diese Person auch als Autor genannt wird.
Dies ist jedoch nicht garantiert, da die Abbildung gleichzeitig zeigt, dass nur etwa 50 \% der Autoren mit den meisten Commits tatsächlich als Autor aufgeführt sind.
Außerdem zeigt sie, dass in einigen Paketen die Autoren mit den meisten Commits oder geänderten Zeilen auch gar nicht als Autor genannt werden können.
Aus diesen Gründen lässt sich annehmen, dass viel Arbeit in einem Projekt ein guter Ansatz ist, um als Autor genannt zu sein.
Dies jedoch keine Garantie dafür genannt zu werden und in den meisten Fällen werden weitere Schritte benötigt, um tatsächlich aufgeführt zu werden.

Ein möglicher Schritt, welcher jedoch nicht umsetzbar ist, ist bei der Gründung des Paketes beteiligt zu sein.
Dies geht aus der \autoref{fig:added_removed_authors} hervor.
Sie zeigt das Problem, dass die meisten Autoren direkt zu Beginn genannt werden und anschließend kaum weitere Autoren hinzugefügt werden und somit die Autorenliste nicht aktiv gepflegt wird.
Insgesamt wurden in allen Paketen der untersuchten Listen nur neun Autoren, der \gls{cff} oder \hologo{BibTeX}-Datei nachträglich hinzugefügt.
\autoref{fig:added_removed_authors_without_readme} zeigt jedoch, dass in den Dateien viel mehr Autoren insgesamt enthalten sind.
Hierbei muss allerdings beachtet werden, dass die \gls{cff}-Datei erst seit 2021 vermehrt verwendet wird, wie aus \autoref{fig:valid_cff_by_time} hervorgeht.
Dadurch sind erst drei Jahre vergangen, in welchen Autoren hinzugefügt werden konnten.
Und diese Autoren müssten in den drei Jahren auch aktiv am Projekt beteiligt gewesen sein, um als Autor genannt zu werden.
Ebenfalls spiegelt die \autoref{fig:total_authors_no_commits} dieses Verhalten wider.
Hier wird deutlich, dass viele genannte Autoren keinen Commit in den letzten Jahren getätigt haben.
Jedoch muss berücksichtigt werden, dass inaktive Repositorys mit in diese Statistik einfließen, welche ebenfalls Autoren enthalten, welche nicht mehr aktiv am Projekt beteiligt sind, da das Projekt eingestellt wurde.
Auf die Inaktivität von Autoren und deren Pflege wird im nächsten Abschnitt genauer eingegangen.

\section{Wie gut werden Autoren in den einzelnen Quellen gepflegt?}
\label{sec:autoren_pflege_diskussion}
In \autoref{fig:common_authors_2} wurde gezeigt, dass viele Autoren gar nicht unter den Top-100-Git-Autoren sind.
Dies bedeutet, dass viele der genannten Autoren gar nicht mehr aktiv am Projekt beteiligt sind.
Dies kann verschiedene Gründe haben, wie beispielsweise, dass die Autoren das Projekt verlassen haben.
Außerdem muss beachtet werden, dass Autoren, welche nicht in der Datenbeschaffung abgeglichen werden konnten, hier ebenfalls enthalten sind.
Die Abbildung deutet allerdings bereits darauf hin, dass Autoren, sobald sie einmal eingetragen wurden, nicht mehr entfernt werden, obwohl sie nicht mehr aktiv am Projekt beteiligt sind.
Zudem zeigt es, in Verbindung mit \autoref{fig:common_authors}, dass Autoren mit vielen Commits ebenfalls kaum genannt werden, was die Vermutung bestätigt, dass die Autorenlisten in den meisten Fällen nicht aktiv gepflegt werden.

Ein weiterer Indikator dafür ist die \autoref{fig:total_authors_no_commits}.
Hier wird deutlich, dass viele genannte Autoren in den letzten Jahren keinen Commit getätigt haben.
Dies lässt sich allerdings dadurch relativieren, dass die Statistik ebenfalls Pakete enthält, welche nicht mehr aktiv entwickelt werden, was allerdings bei den Top-100-Listen unwahrscheinlich ist.
Des Weiteren ist die hohe Anzahl der invaliden \gls{cff}-Dateien, welche in \autoref{fig:overall_valid_cff} und \autoref{fig:valid_cff_by_time} deutlich werden, ein Indikator dafür, dass die Pflege der Autoren den Entwicklern der Pakete nicht besonders wichtig zu scheinen sei.

Auch zeigt die Häufigkeit, mit der die Quellen aktualisiert werden, dass scheinbar kein großes Interesse darin besteht, die Autorenlisten zu pflegen.
Aus \autoref{tab:average_time_last_update} wird deutlich, dass zwei der drei untersuchten Quellen in fast jeder Liste durchschnittlich das letzte Jahr nicht aktualisiert wurden.
Die README wird dabei öfter aktualisiert, wobei berücksichtigt werden muss, dass in der README nicht nur Autoren, sondern auch in vielen Fällen beispielsweise die Dokumentation vorhanden ist.
Außerdem muss berücksichtigt werden, dass ein Jahr in der Softwareentwicklung keine lange Zeit ist und neue Autoren innerhalb dieser Zeit kaum hinzugefügt werden können, da ein Einarbeiten und etablieren in ein großes Softwareprojekt innerhalb eines Jahres schwer möglich ist.
Dahingegen ist dies bei der \hologo{BibTeX} Quelle anders, da hier die letzte Aktualisierung zwei bis drei Jahre zurückliegt in der die Autorenliste ggf. um weitere Autoren ergänzt hätte werden können.

\autoref{fig:similarities} zeigt, dass die Übereinstimmung der Autoren über die Quellen hinweg gering ist.
Besonders in der \gls{pypi} \gls{cff} Liste wird dies deutlich.
Dabei werden allerdings auch Quellen wie die README betrachtet, in welcher bei vielen Paketen keine oder kaum Autoren genannt werden.
Auch ist die Abbildung erneut stark abhängig von dem Abgleich in der Datenbeschaffung.
Allerdings lässt sich hier erneut erkennen, dass die Autoren in den Quellen nicht automatisch gepflegt werden.
Des Weiteren zeigen die Tabellen \ref{tab:average_lifespans} und \ref{tab:average_lifespans_until_today}, sowie die \autoref{fig:added_removed_authors}, dass einmal hinzugefügte Autoren in den meisten Fällen nicht mehr entfernt werden.
Diese Tatsache ist dabei nichts Negatives, da die Autoren Arbeit in den Paketen geleistet haben, allerdings sollten Autoren, welche aktuell das Paket aktiv pflegen ebenfalls genannt werden.

Im Allgemeinen lässt sich sagen, dass die Autoren in den betrachteten Listen nicht aktiv gepflegt werden und besonders in den meisten Fällen keine automatischen Prozesse vorhanden sind, welche die Autorenlisten aktualisieren.
Dies könnte unter anderem daran liegen, dass viele der Pakete nicht in der Wissenschaft entstanden sind, sondern beispielsweise in Unternehmen, bei denen die Pflege der Autorenlisten nicht so wichtig ist, sondern die Nennung des Unternehmens im Fokus steht.

Eine zusätzliche Auffälligkeit, welche die Pflege und Nennung der Autoren indirekt betrifft, zeigt \autoref{fig:citation_counts}.
Hier wird deutlich, dass in vielen Fällen, in denen eine \emph{preferred-citation} angegeben ist, diese auf ein Paper verweist und nicht auf die Software.
Falls Autoren von wissenschaftlichen Arbeiten ausschließlich diese Referenz zitieren, stellt dies einen Verstoß gegen das Prinzip der Wichtigkeit dar, welches in \autoref{sec:software-zitation} beschrieben wurde.
Dies könnte durch die Autoren von Software verhindert werden, indem sie keine \emph{preferred-citation} angeben, sondern ausschließlich die Software und weitere Referenzen in der \gls{cff} als \emph{references} angeben.
Diese Referenzen wurden in dieser Masterarbeit allerdings nicht untersucht.
